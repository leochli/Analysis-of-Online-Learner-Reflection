----------------------- Page 1-----------------------

       IERG3320/ESTR3306IERG3320/ESTR3306 
  Social Media and Human  
   Information Interactions 

Week Eight: Semantic Information (II)Week Eight: Semantic Information (II) 

              Prof. Rosanna Y.-Y. Chan 
                 27 October 2017 

----------------------- Page 2-----------------------

        Levels of Linguistic Structure 

•   Major levels of linguistic structure, from raw spoken sounds to the  
   complexities of meanings influenced by context (Hinton, Ch. 8). 
                                                                                       2 

----------------------- Page 3-----------------------

               The Written Word 

•  Writing changes language from an oral variant event  
   to a writtento a written invariant invariant object.object. 

•  Writing enabled us to freeze our ideas in time and  
   space and then dissect and study them. It also enables  
   us to create logically structured, transmittable, stored  
   and retrieved semantic information. 

•  This turn brings with it the ability to organize  
   information in ways different from linear, ephemeral  
   speech. 

                                                                              3 

----------------------- Page 4-----------------------

              The Written Word 

•  When we read the written word, we are picking  
   up information from the marks on a surface up information from the marks on a surface  
   representing the bodily sounds we make when  
   speaking; and because we’ve been taught  
   (usually from an early age) how to decode these  
   marks, we have learned how to translate the  
   physical affordance of “seeing marks on a physical affordance of “seeing marks on a  
   surface” into the mediated meaning we get from  
   “reading.” 

                                                                           4 

----------------------- Page 5-----------------------

                                List 

•  Lists are one of the  
   oldest and most oldest and most  
   useful forms of  
   written language.  

••  They make labels They make labels  
   names of things  
   into structures. 
                                      A cuneiform tablet listing a temple’s  
                                      possessions (shown at the Louvre, Paris). 

                                                                                 5 

----------------------- Page 6-----------------------

          The Structure of Writing 

•   Writing gives us the ability to put what we say onto a  
    surface and analyze it more conveniently than oral surface and analyze it more conveniently than oral  
    speech.speech. 
•   Besides, many natural languages have developed  
    standardized grammatical conventions.  

  A sentence about elementary school children misbehaving in a  
  performance, broken into a standard English sentence diagram format. 
                                                                                           6 

----------------------- Page 7-----------------------

                Content Analysis 

•  Content analysis is a broad term that sums up  
   nearly every nearly every analytical techniqueanalytical technique that can be that can be  
   used to extract secondary meaning from 
   contents generated by human communication 
    – Can refer to both human and computer techniques. 

••  Computerized content analysis Computerized content analysis leverages large leverages large  
   quantities of cheap computing power to perform  
   analyses previously unthinkable at scales  
   formerly intractable. 

                                                                             7 

----------------------- Page 8-----------------------

 Natural Language Processing (NLP) 

•  NLP involves making the computer understand  
   natural language input natural language input and and generate natural generate natural  
   language output. 
•  It is a field that involves computer science, linguistics,  
   artificial intelligence, human-computer interface, etc. 

                                                                                 8 

----------------------- Page 9-----------------------

 Natural Language Processing (NLP) 

•   Examples of NLP tasks in Social Media Analysis 
    –– Topic analysis Topic analysis ––What are the people talking about?What are the people talking about? 
    – Trend analysis –Which topic is gaining popularity  
        over time? 
    – Sentiment analysis – What do people think about a  
        person / company / product / government? 
    –– Information flow analysis Information flow analysis –– How does a particular How does a particular  
        piece of information flow in a social network?  
        (Combined with social network analysis.) 

                                                                                    9 

----------------------- Page 10-----------------------

   Sentiment Analysis and Opinion  
                         Mining 

•  Sentiment analysis and opinion mining is the  
   field of study involvingfield of study involving information retrieval and information retrieval and  
   computational linguistics that analyzes people’s  
   opinions, sentiments, evaluations, appraisals,  
   attitudes, and emotions towards entities such as  
   products, services, organizations, individuals,  
   issues, events, topics, and their attributes. 

•  Done by content analysis and Natural Language  
   Processing. 
    – Supported by Python. 

                                                                     10 

----------------------- Page 11-----------------------

      Example of Online Content: 
             Product Reviews  

                                     Opinion holder 

                           Opinion 
http://www.epinions.com                                11 

----------------------- Page 12-----------------------

 Natural Language Processing (NLP) 

•  How is NLP done? 

                                                                        12 

----------------------- Page 13-----------------------

                        Pre-Processing 

   •  The first step in any content analysis project is  
       the collection and preparation of the data to the collection and preparation of the data to  
       be analyzed. 

E.g., your  
                                                                              E.g., .txt  
blog  
                                                                              files. 
comments. 

                 (Image source: ccm.net)                 (Image source: blogs.mtu.edu) 

                                                                                         13 

----------------------- Page 14-----------------------

                   Pre-Processing 

•  All text data need some cleaning before they  
   can be analyzed. E.g.,:can be analyzed. E.g.,: 
    – Remove HTML / XML markup tags 
    – Filter non-language characters (e.g. emoticons) 
    – Filter sentences that are in other languages 
    –– Remove stop words (e.g. a, an, the, of, in, to, …)Remove stop words (e.g. a, an, the, of, in, to, …) 
    – Normalization (upper/lowercase, spelling, word  
       forms, …) 

                                                                               14 

----------------------- Page 15-----------------------

                      Tokenization 

•  A token is a unit in natural language processing 
    –– Usually it is a word in English (or other languages Usually it is a word in English (or other languages  
        using Latin alphabets), separated by space. 
    – Tokenization = breaking up the raw text into words  
        (or other meaningful units). 

•   Problems to be dealt with in tokenization: 
    –– How do we treat punctuations?How do we treat punctuations? 
    – John’s book (John + ‘s + book ?) 
    – Doesn’t (does + not, or does + n’t ?) 
    – Hyphenated words: so-called, high-risk 

                                                                                   15 

----------------------- Page 16-----------------------

                   Normalization 

•  Words in uppercase or lowercase 
    –– E.g. Usually we do not want to treat ‘house’, E.g. Usually we do not want to treat ‘house’,  
       ‘House’ and ‘HOUSE’ differently 
•  Normally, we convert all words into  
   lowercases (lowercasing) 

••  TruecasingTruecasing:: try to preserve uppercase in try to preserve uppercase in  
   entity names, in order to distinguish between  
   something like ‘Mr. Brown’ and ‘brown  
   colour’. 

                                                                          16 

----------------------- Page 17-----------------------

                        Stemming 

•  A word may appear in different forms,  
   consider:consider: 
    – cat, cats / bus, buses 
    – run, running, runs 
    – fun, funny / beautiful, beautifully 

••  Stemming is the action of reducing words to Stemming is the action of reducing words to  
   its ‘stem’ or ‘root’ 
    – e.g. cat, cats cat ; run, running, runs run 

                                                                              17 

----------------------- Page 18-----------------------

                        Stemming 

•  The widely used stemming method used is the Porter  
   StemmerStemmer, invented by Martin F. Porter in 1980., invented by Martin F. Porter in 1980. 
    –  http://tartarus.org/martin/PorterStemmer/ 
    – The stemmer is available in many different programming  
       languages (e.g. C, C++, Python, Java, etc.) 

•  Exemple: 

                                                                                18 

----------------------- Page 19-----------------------

                  Parts of Speech 

•  Words have different roles in a sentence: 
    –– nounsnouns (e.g. house, car, people)(e.g. house, car, people) 
    – verbs (e.g. run, walk, pay, eat) 
    – adjectives (e.g. beautiful, quick) 

•  Roughly, we can divide words into two broad  
   categories:categories: 
    – Content words (e.g. nouns, verbs) 
    – Function words (e.g. prepositions) 

                                                                               19 

----------------------- Page 20-----------------------

                  Parts of Speech 

•  Content words are also called open-class  
   wordswords (not a finite set of words, word can be (not a finite set of words, word can be  
   created or become obsolete) 

•  Function words are called close-class words,  
   because usually they do not change over a because usually they do not change over a  
   long period of time. 

                                                                             20 

----------------------- Page 21-----------------------

        Parts of Speech (POS) Tags 

•   A universal POS tagset defines 12 POS tags, which exist in similar  
    form in most languages: 
     ––  NOUN (nouns),NOUN (nouns), 
     –   VERB (verbs), 
     –   ADJ (adjectives), 
     –   ADV (adverbs), 
     –   PRON (pronouns), 
     –   DET (determiners and articles), 
     –   ADP (prepositions and postpositions), 
     ––  NUM (numerals),NUM (numerals), 
     –   CONJ (conjunctions), 
     –   PRT (particles), 
     –   ‘.’ (punctuation marks) and 
     –   X (a catch-all for other categories such as abbreviations or foreign  
         words) 

                                                                                                21 

----------------------- Page 22-----------------------

      Parts of Speech (POS) Tags 

•  The two most common words for some POS  
   tags in the new Google Books NGram Corpus tags in the new Google Books NGram Corpus  
   for all languages 

                                                                            22 

----------------------- Page 23-----------------------

      Parts of Speech (POS) Tags 

•  Usage in annotated Ngrams (Lin et al., 2012) 

                                                                        23 

----------------------- Page 24-----------------------

                Language Model 

•  Language models (LMs) captures the  
   characteristics of a language.characteristics of a language. 
•  LMs are probability distributions of sequence  
   of words; and enable us to make predictions  
   of the next words appearing in a sequence. 
••  LMs are LMs are ‘generative models’: we can use a LM ‘generative models’: we can use a LM  
   to generate texts that would possess the  
   characteristics of the language from which an  
   LM is learnt. 

                                                                        24 

----------------------- Page 25-----------------------

                         N-grams 

•  Different LMs can be constructed by making  
   different assumptions.different assumptions. 

•  Unigram Models 
    – All words in a document are generated  
       independently. 
•  Bigram Models 
    –– The probability distribution of the next word depends The probability distribution of the next word depends  
       on the previous word. 
•  Trigram Models 
    – The probability distribution of the next word depends  
       on the previous TWO words. 

                                                                            25 

----------------------- Page 26-----------------------

                                   N-grams 

•    P( w      | w  w …w           ) is called a parameter of the language  
            n       1   2       n-1 
     model 
••  To estimating the values of the parameters of an NTo estimating the values of the parameters of an N--gram gram  
     model from the training data: 

                                                               C(w ) = count of occurrence of w 
                                                                     i                                    i 

                                                               N = total number of words in the  
                                                               training data 

                                                                                                           26 

----------------------- Page 27-----------------------

                        N-grams 

•  E.g. given the training data: 

          Happy birthday to you Happy birthday to  
          you Happy birthday to Mary Happy  
          birthday to you 

•  What can we tell about the probability of  
   occurrence of “you” based on the training occurrence of “you” based on the training  
   data? 

                                                                         27 

----------------------- Page 28-----------------------

                             N-grams 

•   E.g., For unigram model, P(you) = 
          C(you) / N = 3/16C(you) / N = 3/16 

•   For bigram model, P(you|to) =  
          C(“to you”) / C(to) = 3/4 

•   For trigram model, P(you|birthday to) =  
          C(“birthday to you”) / C(“birthday to”) C(“birthday to you”) / C(“birthday to”)  
          = 3/4 

•   How about P(Mary | Happy birthday to)? 
          Answer: ¼.  

                                                                                        28 

----------------------- Page 29-----------------------

  Google Book’s N-Gram Viewer 

•  Based on number of counts of the searched  
   words or phrases.words or phrases. 
•  Support phrases with length up to five words. 
•  Smoothing 
    – Smoothing by averaging the results on years before  
       and after  
•  Google’s N-Gram Viewer 
    –– https://books.google.com/ngrams https://books.google.com/ngrams  

                                                                              29 

----------------------- Page 30-----------------------

     Google Book’s N-Gram Viewer 

   •  https://books.google.com/ngrams/info 

E.g., Y-axis shows, of all the bigrams contained in Googles’ sample of books written  
in English and published in the United States between 1950 and 2000, what  
percentage of them are “nursery school” or “child care” (under the bigram model). 

                                                                                    30 

----------------------- Page 31-----------------------

                           References 

•   Google’s N-gram 
     –– https://books.google.com/ngramshttps://books.google.com/ngrams 

•   Y. Lin et al., Syntactic Annotations for the Google Books  
    Ngram Corpus, Proceedings of the 50th Annual Meeting of  
    the Association for Computational Linguistics, pp. 169– 174,  
    July 2012. 
•   K. H. Leetaru, Data Mining Methods for the Content Analyst:  
    An Introduction to the Computational Analysis of ContentAn Introduction to the Computational Analysis of Content, ,  
    Routledge, New York, NY, 2012. 

                                                                                            31 
